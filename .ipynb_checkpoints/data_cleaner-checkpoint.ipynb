{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46955421d76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#from nltk.stem import PorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import emoji\n",
    "#from nltk.stem import PorterStemmer\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Removes Mentions and https and hastags\n",
    "    clean_text = re.sub(r\"(?:\\@|https?\\://|#\\w+[\\S\\n]+)\\S+\", \"\", text)\n",
    "    #print(clean_text)\n",
    "    \n",
    "    return clean_text #UNUSED SINCE STRING IS STILL A STRING \"\".join(filtered_sentence)\n",
    "\n",
    "    #Removes emoji's MOVED TO BE CLEANED AFTER\n",
    "    #emoji_free = emoji.get_emoji_regexp().sub(u'', clean_text)\n",
    "    #print(emoji_free)\n",
    "    \n",
    "    #decontracts words MOVED TO BE CLEANED LATER\n",
    "    #decon_text = decontracted(emoji_free)\n",
    "    #print(decon_text)\n",
    "    \n",
    "    #Removes stopwords UNUSED BECAUSE OF SENTIMENT ANALYSIS\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #print(stop_words)\n",
    "    #word_tokens = word_tokenize(decon_text) \n",
    "    #punctuation = [\",\", \".\", \"\\'\", \"\\'s\"]\n",
    "  \n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    #filtered_sentence = [] \n",
    "  \n",
    "    #for w in word_tokens: \n",
    "    #    if w not in stop_words:# and w not in punctuation: \n",
    "    #        filtered_sentence.append(w)\n",
    "    #        filtered_sentence.append(\" \")\n",
    "    #print(filtered_sentence)\n",
    "\n",
    "    #THIS WAS TOO STRONG SO IT WAS COMMENTED OUT\n",
    "    #Stems the remaining words T\n",
    "    #porter = PorterStemmer()\n",
    "    #stem_sentence = []\n",
    "    #for word in filtered_sentence:\n",
    "    #    stem_sentence.append(porter.stem(word))\n",
    "    #    stem_sentence.append(\" \")\n",
    "    #print(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence @ENOCHCHCHCHCHH ðŸ˜ª, #DYING https://asdlkfjldksjf.com shouldn't #DYING couldn't #LIVING wouldn't he'll showing's off the stop words' filtration. https://askdfhkjasdfh.com #DYE \n",
      "This is a sample sentence  ðŸ˜ª,   shouldn't  couldn't  wouldn't he'll showing's off the stop words' filtration.   \n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence @ENOCHCHCHCHCHH \\U0001f62a, #DYING https://asdlkfjldksjf.com shouldn't #DYING couldn't #LIVING wouldn't he'll showing's off the stop words' filtration. https://askdfhkjasdfh.com #DYE \"\n",
    "print(example_sent)\n",
    "temp = clean_text(example_sent)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_info(line):\n",
    "    tweet_obj = json.loads(line)\n",
    "    try:\n",
    "        if tweet_obj[\"coordinates\"] is None and tweet_obj[\"place\"] is None:\n",
    "             return \"NOLOC\\n\"\n",
    "        text = tweet_obj[\"text\"]\n",
    "        #CLEAN THE TWEET\n",
    "        #print(text)\n",
    "        cleaned_text = clean_text(text)\n",
    "        #print(cleaned_text)\n",
    "\n",
    "        hashtags = tweet_obj[\"entities\"][\"hashtags\"]\n",
    "\n",
    "        if tweet_obj[\"coordinates\"] is not None:\n",
    "            point_coordinates = tweet_obj[\"coordinates\"][\"coordinates\"]\n",
    "        else:\n",
    "            point_coordinates = \"Null\"\n",
    "\n",
    "        if tweet_obj[\"place\"] is not None:\n",
    "            place = tweet_obj[\"place\"][\"full_name\"]\n",
    "            box_coordinates = tweet_obj[\"place\"][\"bounding_box\"][\"coordinates\"]\n",
    "        else:\n",
    "            place = \"Null\"\n",
    "            box_coordinates = \"Null\"\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "\n",
    "    clean = {}\n",
    "    clean['text'] = cleaned_text\n",
    "    clean['hashtags'] = hashtags\n",
    "    clean['point_coordinates'] = point_coordinates\n",
    "    clean['box_coordinates'] = box_coordinates\n",
    "    clean['place'] = place\n",
    "    \n",
    "    result = json.dumps(clean)\n",
    "    result = result + \"\\n\"\n",
    "    return result\n",
    "#endef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('cs226-proj').setMaster('local')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base = \"file:///Users\\jshin\\Documents\\CS_226\\Project\\\\\"\n",
    "#inputfile = \"fartgate_tweets.json\"\n",
    "#inputfile = input() #in form filename.json\n",
    "#filename = base + inputfile\n",
    "#print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMA_tweets.json\n"
     ]
    }
   ],
   "source": [
    "#SPARK VERSION\n",
    "base = \"file:///Users\\jshin\\Documents\\CS_226\\Project\\\\\"\n",
    "#inputfile = \"fartgate_tweets.json\"\n",
    "inputfile = input() #in form filename.json\n",
    "filename = base + inputfile\n",
    "#print(filename)\n",
    "        \n",
    "tweet_rdd = sc.textFile(filename)\n",
    "\n",
    "cleanish_tweets = tweet_rdd.map(lambda x: remove_extra_info(x))\n",
    "clean_rdd = cleanish_tweets.filter(lambda x: x != \"NOLOC\\n\")\n",
    "\n",
    "final = clean_rdd.collect()\n",
    "\n",
    "ofile = 'Clean_' + inputfile\n",
    "with open(ofile, 'w+') as clean_tweets:\n",
    "    for line in final:\n",
    "        clean_tweets.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential METHOD\n",
    "filename = input()\n",
    "tweet_gen = open(filename)\n",
    "ofile = 'Clean_' + filename\n",
    "clean_tweets = open(ofile, 'w+')\n",
    "place_cnt = 0\n",
    "coord_cnt = 0\n",
    "total = 0\n",
    "place_flg = 0\n",
    "coord_flg = 0\n",
    "both_cnt = 0\n",
    "for tweet in tweet_gen:\n",
    "    #print(tweet)\n",
    "    total += 1\n",
    "    place_flg = 0\n",
    "    coord_flg = 0\n",
    "    tweet_obj = json.loads(tweet)\n",
    "    try:\n",
    "        if tweet_obj[\"coordinates\"] is None and tweet_obj[\"place\"] is None:\n",
    "            continue #remove tweets with no location data\n",
    "        text = tweet_obj[\"text\"]\n",
    "        #CLEAN THE TWEET\n",
    "        #print(text)\n",
    "        cleaned_text = clean_text(text)\n",
    "        #print(cleaned_text)\n",
    "        \n",
    "        #if tweet_obj[\"entities\"][\"hashtags\"] is not None:\n",
    "        hashtags = tweet_obj[\"entities\"][\"hashtags\"]\n",
    "        #else:\n",
    "        #    hashtags = \"Null\"\n",
    "        \n",
    "        if tweet_obj[\"coordinates\"] is not None:\n",
    "            point_coordinates = tweet_obj[\"coordinates\"][\"coordinates\"]\n",
    "            coord_cnt += 1\n",
    "            coord_flg = 1\n",
    "        else:\n",
    "            point_coordinates = \"Null\"\n",
    "            \n",
    "        if tweet_obj[\"place\"] is not None:\n",
    "            place = tweet_obj[\"place\"][\"full_name\"]\n",
    "            box_coordinates = tweet_obj[\"place\"][\"bounding_box\"][\"coordinates\"]\n",
    "            place_cnt += 1\n",
    "            place_flg = 1\n",
    "        else:\n",
    "            place = \"Null\"\n",
    "            box_coordinates = \"Null\"\n",
    "            \n",
    "        if (place_flg == 1 and coord_flg == 1):\n",
    "            both_cnt += 1\n",
    "            \n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "    \n",
    "    clean = {}\n",
    "    clean['text'] = cleaned_text\n",
    "    clean['hashtags'] = hashtags\n",
    "    clean['point_coordinates'] = point_coordinates\n",
    "    clean['box_coordinates'] = box_coordinates\n",
    "    clean['place'] = place\n",
    "    \n",
    "    json.dump(clean,clean_tweets)\n",
    "    clean_tweets.write(\"\\n\") #adds newlines\n",
    "    #sanity prints:\n",
    "    if ((total+50000)% 50000) == 0:\n",
    "        print(total, \" Tweets Cleaned\")\n",
    "        \n",
    "#end for\n",
    "tweet_gen.close()\n",
    "clean_tweets.close()\n",
    "print(\"Coord count: \", coord_cnt)\n",
    "print(\"Place count: \", place_cnt)\n",
    "print(\"Both count: \", both_cnt)\n",
    "print(\"Total Tweets: \", total)\n",
    "print(\"Percentage Locations: \", ((place_cnt+coord_cnt)-both_cnt)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coord count:  7406\n",
      "Place count:  113845\n",
      "Both count:  7406\n",
      "Total Tweets:  131285\n"
     ]
    }
   ],
   "source": [
    "#OLD METHOD\n",
    "tweet_gen = open('fartgate_tweets.json')\n",
    "clean_tweets = open('Clean_fartgate_tweets.json', 'w+')\n",
    "place_cnt = 0\n",
    "coord_cnt = 0\n",
    "total = 0\n",
    "place_flg = 0\n",
    "coord_flg = 0\n",
    "both_cnt = 0\n",
    "for tweet in tweet_gen:\n",
    "    #print(tweet)\n",
    "    total += 1\n",
    "    place_flg = 0\n",
    "    coord_flg = 0\n",
    "    tweet_obj = json.loads(tweet)\n",
    "    try:\n",
    "        text = tweet_obj[\"text\"]\n",
    "        \n",
    "        #if tweet_obj[\"entities\"][\"hashtags\"] is not None:\n",
    "        hashtags = tweet_obj[\"entities\"][\"hashtags\"]\n",
    "        #else:\n",
    "        #    hashtags = \"Null\"\n",
    "        \n",
    "        if tweet_obj[\"coordinates\"] is not None:\n",
    "            point_coordinates = tweet_obj[\"coordinates\"][\"coordinates\"]\n",
    "            coord_cnt += 1\n",
    "            coord_flg = 1\n",
    "        else:\n",
    "            point_coordinates = \"Null\"\n",
    "            \n",
    "        if tweet_obj[\"place\"] is not None:\n",
    "            place = tweet_obj[\"place\"][\"full_name\"]\n",
    "            box_coordinates = tweet_obj[\"place\"][\"bounding_box\"][\"coordinates\"]\n",
    "            place_cnt += 1\n",
    "            place_flg = 1\n",
    "        else:\n",
    "            place = \"Null\"\n",
    "            box_coordinates = \"Null\"\n",
    "            \n",
    "        if (place_flg == 1 and coord_flg == 1):\n",
    "            both_cnt += 1\n",
    "            \n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "    \n",
    "    clean = {}\n",
    "    clean['text'] = text\n",
    "    clean['hashtags'] = hashtags\n",
    "    clean['point_coordinates'] = point_coordinates\n",
    "    clean['box_coordinates'] = box_coordinates\n",
    "    clean['place'] = place\n",
    "    \n",
    "    json.dump(clean,clean_tweets)\n",
    "    \n",
    "#end for\n",
    "tweet_gen.close()\n",
    "clean_tweets.close()\n",
    "print(\"Coord count: \", coord_cnt)\n",
    "print(\"Place count: \", place_cnt)\n",
    "print(\"Both count: \", both_cnt)\n",
    "print(\"Total Tweets: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
